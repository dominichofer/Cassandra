#include "Bit.h"
#include "Field.h"
#include "Position.h"
#include <array>
#include <cstdint>

#pragma once
// Predefined macros:
// __GNUC__           Compiler is gcc.
// __clang__          Compiler is clang.
// __INTEL_COMPILER   Compiler is Intel.
// _MSC_VER           Compiler is Microsoft Visual Studio.
// _M_X64             Microsoft specific macro when targeting 64 bit based machines.
// __x86_64           Defined by GNU C and Sun Studio when targeting 64 bit based machines.

#if defined(_MSC_VER)
#include <intrin.h>
#elif defined(__GNUC__)
#include <x86intrin.h>
#else
#error compiler not supported!
#endif

#ifdef __CUDA_ARCH__
	__constant__ static const uint64 masks[1024] = {
#else
	static const std::array<int64_t, 1024> masks = {
#endif

	0x00000000000000feLL, 0x0101010101010100LL, 0x8040201008040200LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x00000000000000fcLL, 0x0202020202020200LL, 0x0080402010080400LL, 0x0000000000000100LL, 0x0000000000000001LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x00000000000000f8LL, 0x0404040404040400LL, 0x0000804020100800LL, 0x0000000000010200LL, 0x0000000000000003LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x00000000000000f0LL, 0x0808080808080800LL, 0x0000008040201000LL, 0x0000000001020400LL, 0x0000000000000007LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x00000000000000e0LL, 0x1010101010101000LL, 0x0000000080402000LL, 0x0000000102040800LL, 0x000000000000000fLL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x00000000000000c0LL, 0x2020202020202000LL, 0x0000000000804000LL, 0x0000010204081000LL, 0x000000000000001fLL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x0000000000000080LL, 0x4040404040404000LL, 0x0000000000008000LL, 0x0001020408102000LL, 0x000000000000003fLL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x0000000000000000LL, 0x8080808080808000LL, 0x0000000000000000LL, 0x0102040810204000LL, 0x000000000000007fLL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL,
	0x000000000000fe00LL, 0x0101010101010000LL, 0x4020100804020000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000001LL, 0x0000000000000000LL, 0x0000000000000002LL,
	0x000000000000fc00LL, 0x0202020202020000LL, 0x8040201008040000LL, 0x0000000000010000LL, 0x0000000000000100LL, 0x0000000000000002LL, 0x0000000000000001LL, 0x0000000000000004LL,
	0x000000000000f800LL, 0x0404040404040000LL, 0x0080402010080000LL, 0x0000000001020000LL, 0x0000000000000300LL, 0x0000000000000004LL, 0x0000000000000002LL, 0x0000000000000008LL,
	0x000000000000f000LL, 0x0808080808080000LL, 0x0000804020100000LL, 0x0000000102040000LL, 0x0000000000000700LL, 0x0000000000000008LL, 0x0000000000000004LL, 0x0000000000000010LL,
	0x000000000000e000LL, 0x1010101010100000LL, 0x0000008040200000LL, 0x0000010204080000LL, 0x0000000000000f00LL, 0x0000000000000010LL, 0x0000000000000008LL, 0x0000000000000020LL,
	0x000000000000c000LL, 0x2020202020200000LL, 0x0000000080400000LL, 0x0001020408100000LL, 0x0000000000001f00LL, 0x0000000000000020LL, 0x0000000000000010LL, 0x0000000000000040LL,
	0x0000000000008000LL, 0x4040404040400000LL, 0x0000000000800000LL, 0x0102040810200000LL, 0x0000000000003f00LL, 0x0000000000000040LL, 0x0000000000000020LL, 0x0000000000000080LL,
	0x0000000000000000LL, 0x8080808080800000LL, 0x0000000000000000LL, 0x0204081020400000LL, 0x0000000000007f00LL, 0x0000000000000080LL, 0x0000000000000040LL, 0x0000000000000000LL,
	0x0000000000fe0000LL, 0x0101010101000000LL, 0x2010080402000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000101LL, 0x0000000000000000LL, 0x0000000000000204LL,
	0x0000000000fc0000LL, 0x0202020202000000LL, 0x4020100804000000LL, 0x0000000001000000LL, 0x0000000000010000LL, 0x0000000000000202LL, 0x0000000000000100LL, 0x0000000000000408LL,
	0x0000000000f80000LL, 0x0404040404000000LL, 0x8040201008000000LL, 0x0000000102000000LL, 0x0000000000030000LL, 0x0000000000000404LL, 0x0000000000000201LL, 0x0000000000000810LL,
	0x0000000000f00000LL, 0x0808080808000000LL, 0x0080402010000000LL, 0x0000010204000000LL, 0x0000000000070000LL, 0x0000000000000808LL, 0x0000000000000402LL, 0x0000000000001020LL,
	0x0000000000e00000LL, 0x1010101010000000LL, 0x0000804020000000LL, 0x0001020408000000LL, 0x00000000000f0000LL, 0x0000000000001010LL, 0x0000000000000804LL, 0x0000000000002040LL,
	0x0000000000c00000LL, 0x2020202020000000LL, 0x0000008040000000LL, 0x0102040810000000LL, 0x00000000001f0000LL, 0x0000000000002020LL, 0x0000000000001008LL, 0x0000000000004080LL,
	0x0000000000800000LL, 0x4040404040000000LL, 0x0000000080000000LL, 0x0204081020000000LL, 0x00000000003f0000LL, 0x0000000000004040LL, 0x0000000000002010LL, 0x0000000000008000LL,
	0x0000000000000000LL, 0x8080808080000000LL, 0x0000000000000000LL, 0x0408102040000000LL, 0x00000000007f0000LL, 0x0000000000008080LL, 0x0000000000004020LL, 0x0000000000000000LL,
	0x00000000fe000000LL, 0x0101010100000000LL, 0x1008040200000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000010101LL, 0x0000000000000000LL, 0x0000000000020408LL,
	0x00000000fc000000LL, 0x0202020200000000LL, 0x2010080400000000LL, 0x0000000100000000LL, 0x0000000001000000LL, 0x0000000000020202LL, 0x0000000000010000LL, 0x0000000000040810LL,
	0x00000000f8000000LL, 0x0404040400000000LL, 0x4020100800000000LL, 0x0000010200000000LL, 0x0000000003000000LL, 0x0000000000040404LL, 0x0000000000020100LL, 0x0000000000081020LL,
	0x00000000f0000000LL, 0x0808080800000000LL, 0x8040201000000000LL, 0x0001020400000000LL, 0x0000000007000000LL, 0x0000000000080808LL, 0x0000000000040201LL, 0x0000000000102040LL,
	0x00000000e0000000LL, 0x1010101000000000LL, 0x0080402000000000LL, 0x0102040800000000LL, 0x000000000f000000LL, 0x0000000000101010LL, 0x0000000000080402LL, 0x0000000000204080LL,
	0x00000000c0000000LL, 0x2020202000000000LL, 0x0000804000000000LL, 0x0204081000000000LL, 0x000000001f000000LL, 0x0000000000202020LL, 0x0000000000100804LL, 0x0000000000408000LL,
	0x0000000080000000LL, 0x4040404000000000LL, 0x0000008000000000LL, 0x0408102000000000LL, 0x000000003f000000LL, 0x0000000000404040LL, 0x0000000000201008LL, 0x0000000000800000LL,
	0x0000000000000000LL, 0x8080808000000000LL, 0x0000000000000000LL, 0x0810204000000000LL, 0x000000007f000000LL, 0x0000000000808080LL, 0x0000000000402010LL, 0x0000000000000000LL,
	0x000000fe00000000LL, 0x0101010000000000LL, 0x0804020000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000001010101LL, 0x0000000000000000LL, 0x0000000002040810LL,
	0x000000fc00000000LL, 0x0202020000000000LL, 0x1008040000000000LL, 0x0000010000000000LL, 0x0000000100000000LL, 0x0000000002020202LL, 0x0000000001000000LL, 0x0000000004081020LL,
	0x000000f800000000LL, 0x0404040000000000LL, 0x2010080000000000LL, 0x0001020000000000LL, 0x0000000300000000LL, 0x0000000004040404LL, 0x0000000002010000LL, 0x0000000008102040LL,
	0x000000f000000000LL, 0x0808080000000000LL, 0x4020100000000000LL, 0x0102040000000000LL, 0x0000000700000000LL, 0x0000000008080808LL, 0x0000000004020100LL, 0x0000000010204080LL,
	0x000000e000000000LL, 0x1010100000000000LL, 0x8040200000000000LL, 0x0204080000000000LL, 0x0000000f00000000LL, 0x0000000010101010LL, 0x0000000008040201LL, 0x0000000020408000LL,
	0x000000c000000000LL, 0x2020200000000000LL, 0x0080400000000000LL, 0x0408100000000000LL, 0x0000001f00000000LL, 0x0000000020202020LL, 0x0000000010080402LL, 0x0000000040800000LL,
	0x0000008000000000LL, 0x4040400000000000LL, 0x0000800000000000LL, 0x0810200000000000LL, 0x0000003f00000000LL, 0x0000000040404040LL, 0x0000000020100804LL, 0x0000000080000000LL,
	0x0000000000000000LL, 0x8080800000000000LL, 0x0000000000000000LL, 0x1020400000000000LL, 0x0000007f00000000LL, 0x0000000080808080LL, 0x0000000040201008LL, 0x0000000000000000LL,
	0x0000fe0000000000LL, 0x0101000000000000LL, 0x0402000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000101010101LL, 0x0000000000000000LL, 0x0000000204081020LL,
	0x0000fc0000000000LL, 0x0202000000000000LL, 0x0804000000000000LL, 0x0001000000000000LL, 0x0000010000000000LL, 0x0000000202020202LL, 0x0000000100000000LL, 0x0000000408102040LL,
	0x0000f80000000000LL, 0x0404000000000000LL, 0x1008000000000000LL, 0x0102000000000000LL, 0x0000030000000000LL, 0x0000000404040404LL, 0x0000000201000000LL, 0x0000000810204080LL,
	0x0000f00000000000LL, 0x0808000000000000LL, 0x2010000000000000LL, 0x0204000000000000LL, 0x0000070000000000LL, 0x0000000808080808LL, 0x0000000402010000LL, 0x0000001020408000LL,
	0x0000e00000000000LL, 0x1010000000000000LL, 0x4020000000000000LL, 0x0408000000000000LL, 0x00000f0000000000LL, 0x0000001010101010LL, 0x0000000804020100LL, 0x0000002040800000LL,
	0x0000c00000000000LL, 0x2020000000000000LL, 0x8040000000000000LL, 0x0810000000000000LL, 0x00001f0000000000LL, 0x0000002020202020LL, 0x0000001008040201LL, 0x0000004080000000LL,
	0x0000800000000000LL, 0x4040000000000000LL, 0x0080000000000000LL, 0x1020000000000000LL, 0x00003f0000000000LL, 0x0000004040404040LL, 0x0000002010080402LL, 0x0000008000000000LL,
	0x0000000000000000LL, 0x8080000000000000LL, 0x0000000000000000LL, 0x2040000000000000LL, 0x00007f0000000000LL, 0x0000008080808080LL, 0x0000004020100804LL, 0x0000000000000000LL,
	0x00fe000000000000LL, 0x0100000000000000LL, 0x0200000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000010101010101LL, 0x0000000000000000LL, 0x0000020408102040LL,
	0x00fc000000000000LL, 0x0200000000000000LL, 0x0400000000000000LL, 0x0100000000000000LL, 0x0001000000000000LL, 0x0000020202020202LL, 0x0000010000000000LL, 0x0000040810204080LL,
	0x00f8000000000000LL, 0x0400000000000000LL, 0x0800000000000000LL, 0x0200000000000000LL, 0x0003000000000000LL, 0x0000040404040404LL, 0x0000020100000000LL, 0x0000081020408000LL,
	0x00f0000000000000LL, 0x0800000000000000LL, 0x1000000000000000LL, 0x0400000000000000LL, 0x0007000000000000LL, 0x0000080808080808LL, 0x0000040201000000LL, 0x0000102040800000LL,
	0x00e0000000000000LL, 0x1000000000000000LL, 0x2000000000000000LL, 0x0800000000000000LL, 0x000f000000000000LL, 0x0000101010101010LL, 0x0000080402010000LL, 0x0000204080000000LL,
	0x00c0000000000000LL, 0x2000000000000000LL, 0x4000000000000000LL, 0x1000000000000000LL, 0x001f000000000000LL, 0x0000202020202020LL, 0x0000100804020100LL, 0x0000408000000000LL,
	0x0080000000000000LL, 0x4000000000000000LL, 0x8000000000000000LL, 0x2000000000000000LL, 0x003f000000000000LL, 0x0000404040404040LL, 0x0000201008040201LL, 0x0000800000000000LL,
	0x0000000000000000LL, 0x8000000000000000LL, 0x0000000000000000LL, 0x4000000000000000LL, 0x007f000000000000LL, 0x0000808080808080LL, 0x0000402010080402LL, 0x0000000000000000LL,
	0xfe00000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0001010101010101LL, 0x0000000000000000LL, 0x0002040810204080LL,
	0xfc00000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0100000000000000LL, 0x0002020202020202LL, 0x0001000000000000LL, 0x0004081020408000LL,
	0xf800000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0300000000000000LL, 0x0004040404040404LL, 0x0002010000000000LL, 0x0008102040800000LL,
	0xf000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0700000000000000LL, 0x0008080808080808LL, 0x0004020100000000LL, 0x0010204080000000LL,
	0xe000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0f00000000000000LL, 0x0010101010101010LL, 0x0008040201000000LL, 0x0020408000000000LL,
	0xc000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x1f00000000000000LL, 0x0020202020202020LL, 0x0010080402010000LL, 0x0040800000000000LL,
	0x8000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x3f00000000000000LL, 0x0040404040404040LL, 0x0020100804020100LL, 0x0080000000000000LL,
	0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x0000000000000000LL, 0x7f00000000000000LL, 0x0080808080808080LL, 0x0040201008040201LL, 0x0000000000000000LL,
};


#ifdef __AVX2__

CUDA_CALLABLE uint64_t Flips(const Position& pos, Field move) noexcept
{
	const __m256i P = _mm256_set1_epi64x(pos.Player());
	const __m256i O = _mm256_set1_epi64x(pos.Opponent());
	const __m256i mask1 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(masks.data() + static_cast<uint8_t>(move) * 8 + 0));
	const __m256i mask2 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(masks.data() + static_cast<uint8_t>(move) * 8 + 4));

	__m256i outflank1 = _mm256_andnot_si256(O, mask1);

	// look for non-opponent LS1B
	outflank1 = _mm256_and_si256(outflank1, neg_epi64(outflank1)); // outflank1 &= -outflank1
	outflank1 = _mm256_and_si256(outflank1, P); // outflank1 &= P
	outflank1 = _mm256_add_epi64(outflank1, not_si256(_mm256_cmpeq_epi64(outflank1, __m256i{0}))); // outflank1 += ~cmpeq(outflank1, 0)
	const __m256i flip1 = _mm256_and_si256(outflank1, mask1); // flip1 = outflank1 & mask1

	// isolate non-opponent MS1B by clearing lower bits
	__m256i outflank2 = _mm256_sllv_epi64(_mm256_and_si256(P, mask2), _mm256_set_epi64x(7, 9, 8, 1));
	__m256i eraser = _mm256_andnot_si256(O, mask2);
	eraser = _mm256_or_si256(eraser, _mm256_srlv_epi64(eraser, _mm256_set_epi64x(7, 9, 8, 1)));
	eraser = _mm256_or_si256(eraser, _mm256_srlv_epi64(eraser, _mm256_set_epi64x(14, 18, 16, 2)));
	eraser = _mm256_or_si256(eraser, _mm256_srlv_epi64(eraser, _mm256_set_epi64x(28, 36, 32, 4)));

	// set mask bits higher than outflank
	const __m256i flip2 = _mm256_and_si256(neg_epi64(_mm256_andnot_si256(eraser, outflank2)), mask2); // flip2 = -andnot(eraser, outflank2) & mask2

	return reduce_or(_mm256_or_si256(flip1, flip2)); // reduce_or(flip1 | flip2)
}

#else

CUDA_CALLABLE BitBoard Flips(const Position& pos, Field move) noexcept
{
	const uint64_t P = pos.Player();
	const uint64_t O = pos.Opponent();
	const uint64_t m = static_cast<uint64_t>(move);

	uint64_t outflank0 = ~O & masks[m * 8 + 0];
	uint64_t outflank1 = ~O & masks[m * 8 + 1];
	uint64_t outflank2 = ~O & masks[m * 8 + 2];
	uint64_t outflank3 = ~O & masks[m * 8 + 3];

	// look for non-opponent LS1B
	outflank0 &= -outflank0;
	outflank1 &= -outflank1;
	outflank2 &= -outflank2;
	outflank3 &= -outflank3;

	outflank0 &= P;
	outflank1 &= P;
	outflank2 &= P;
	outflank3 &= P;

	outflank0 -= static_cast<uint64_t>(outflank0 != 0);
	outflank1 -= static_cast<uint64_t>(outflank1 != 0);
	outflank2 -= static_cast<uint64_t>(outflank2 != 0);
	outflank3 -= static_cast<uint64_t>(outflank3 != 0);

	uint64_t flip = outflank0 & masks[m * 8 + 0]
				| outflank1 & masks[m * 8 + 1]
				| outflank2 & masks[m * 8 + 2]
				| outflank3 & masks[m * 8 + 3];

	// isolate non-opponent MS1B by clearing lower bits
	uint64_t outflank4 = (P & masks[m * 8 + 4]) << 1;
	uint64_t outflank5 = (P & masks[m * 8 + 5]) << 8;
	uint64_t outflank6 = (P & masks[m * 8 + 6]) << 9;
	uint64_t outflank7 = (P & masks[m * 8 + 7]) << 7;

	uint64_t eraser4 = ~O & masks[m * 8 + 4];
	uint64_t eraser5 = ~O & masks[m * 8 + 5];
	uint64_t eraser6 = ~O & masks[m * 8 + 6];
	uint64_t eraser7 = ~O & masks[m * 8 + 7];

	eraser4 |= eraser4 >> 1;
	eraser5 |= eraser5 >> 8;
	eraser6 |= eraser6 >> 9;
	eraser7 |= eraser7 >> 7;

	eraser4 |= eraser4 >> 2;
	eraser5 |= eraser5 >> 16;
	eraser6 |= eraser6 >> 18;
	eraser7 |= eraser7 >> 14;

	eraser4 |= eraser4 >> 4;
	eraser5 |= eraser5 >> 32;
	eraser6 |= eraser6 >> 36;
	eraser7 |= eraser7 >> 28;

	// set mask bits higher than outflank
	return flip | -(~eraser4 & outflank4) & masks[m * 8 + 4]
				| -(~eraser5 & outflank5) & masks[m * 8 + 5]
				| -(~eraser6 & outflank6) & masks[m * 8 + 6]
				| -(~eraser7 & outflank7) & masks[m * 8 + 7];
}
#endif